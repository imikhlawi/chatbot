{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "12be3d78",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello rumble!\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello rumble!\")\n",
        "# PDF-Chatbot – RAG + lokales LLM\n",
        "# Zellen der Reihe nach ausführen (siehe README)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "eccb8a4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"punkt_tab\", quiet=True)\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "502a35cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pfad für den lokalen FAISS-Index (wird beim ersten Lauf erzeugt)\n",
        "FAISS_INDEX_PATH = \"./faiss_index\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2dec9cf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pdf(data):\n",
        "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
        "\n",
        "    documents = loader.load()\n",
        "    \n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "299143ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "extr_data =load_pdf(\"data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cabf61d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "#extr_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8ebf06d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_splitter(extr_data):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    texts = text_splitter.split_documents(extr_data)\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "77831b29",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33\n"
          ]
        }
      ],
      "source": [
        "text_chunks = text_splitter(extr_data)\n",
        "print(len(text_chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "66a66ef3",
      "metadata": {},
      "outputs": [],
      "source": [
        "#text_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a080fae1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_hugging_face_embeddings():\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d00b827c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\imikh\\miniconda3\\envs\\chatbot\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "embeddings = download_hugging_face_embeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5718a37a",
      "metadata": {},
      "outputs": [],
      "source": [
        "#embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "10ffd0d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "query_results = embeddings.embed_query(\"What is the purpose of this document?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "971cf706",
      "metadata": {},
      "outputs": [],
      "source": [
        "#query_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5e5c787f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lokaler Vector-Store mit FAISS (keine Cloud, kein API-Key, keine Migration)\n",
        "doc_search = FAISS.from_documents(text_chunks, embeddings)\n",
        "doc_search.save_local(FAISS_INDEX_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ec8c6d9c",
      "metadata": {},
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for LlamaCpp\n__root__\n  Could not load Llama model from path: llm_model/qwen2.5-coder-32b-instruct-q4_k_m.gguf. Received error Model path does not exist: llm_model/qwen2.5-coder-32b-instruct-q4_k_m.gguf (type=value_error)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Streaming-Callback: Antwort wird während der Generierung angezeigt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager([StreamingStdOutCallbackHandler()])\n\u001b[1;32m----> 9\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllm_model/qwen2.5-coder-32b-instruct-q4_k_m.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\imikh\\miniconda3\\envs\\chatbot\\lib\\site-packages\\langchain\\load\\serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
            "File \u001b[1;32mc:\\Users\\imikh\\miniconda3\\envs\\chatbot\\lib\\site-packages\\pydantic\\main.py:364\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mValidationError\u001b[0m: 1 validation error for LlamaCpp\n__root__\n  Could not load Llama model from path: llm_model/qwen2.5-coder-32b-instruct-q4_k_m.gguf. Received error Model path does not exist: llm_model/qwen2.5-coder-32b-instruct-q4_k_m.gguf (type=value_error)"
          ]
        }
      ],
      "source": [
        "# Lokales LLM laden (Qwen2.5-Coder GGUF) – braucht: pip install llama-cpp-python\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "# Streaming-Callback: Antwort wird während der Generierung angezeigt\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"llm_model/qwen2.5-coder-32b-instruct-q4_k_m.gguf\",\n",
        "    n_ctx=1024,\n",
        "    max_tokens=512,\n",
        "    temperature=0,\n",
        "    repeat_penalty=1.2,\n",
        "    verbose=False,\n",
        "    callback_manager=callback_manager,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de3ac03b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG-Kette: sucht in deiner PDF, antwortet mit dem LLM\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Falls doc_search fehlt (z.B. nach Kernel-Neustart): FAISS-Index von Disk laden\n",
        "try:\n",
        "    doc_search\n",
        "except NameError:\n",
        "    from langchain.vectorstores import FAISS\n",
        "    try:\n",
        "        FAISS_INDEX_PATH\n",
        "    except NameError:\n",
        "        FAISS_INDEX_PATH = \"./faiss_index\"\n",
        "    try:\n",
        "        embeddings\n",
        "    except NameError:\n",
        "        import nltk\n",
        "        nltk.download(\"punkt\", quiet=True)\n",
        "        nltk.download(\"punkt_tab\", quiet=True)\n",
        "        from langchain.embeddings import HuggingFaceEmbeddings\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    doc_search = FAISS.load_local(FAISS_INDEX_PATH, embeddings)\n",
        "    print(\"Vector-Store von Disk geladen.\")\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Prompt: ausführlichere Antwort (mehrere Sätze / kurzer Absatz), nur aus dem Text\n",
        "QA_PROMPT = PromptTemplate(\n",
        "    template=\"\"\"Answer in a short paragraph (4-8 sentences) using ONLY the text below. Be clear and complete. End with a period. Do not invent or repeat. If the answer is not in the text, say: Not in the document.\n",
        "\n",
        "Text:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\",\n",
        "    input_variables=[\"context\", \"question\"],\n",
        ")\n",
        "\n",
        "retriever = doc_search.as_retriever(search_kwargs={\"k\": 2})  # 2 Chunks = weniger Kontext, weniger Verwirrung\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_PROMPT},\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "chatbot",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
